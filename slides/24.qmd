---
engine: knitr
title: Improving performance
---

## Learning Objectives

0.  Determine when to optimize code
1.  Organize code for introducing optimizations
2.  Check for existing solutions
3.  Create simple solutions
4.  Vectorise code
5.  Avoid code copying

```{r}
library(bench)
```

# Determine when to optimize code

## Code that works, is correct, and is reproducible should be the first priority

::: incremental
-   Correct code generates a *good product*
-   Incorrect code generates a *bad product*
-   A good product is *more valuable* than a bad product
-   People will use and wait for valuable products
-   People will not use incorrect products
-   Do not optimize code that doesn't yet generate a good product
:::

## Optimized *correct* code should be the second priority

-   Time and space constraints should set optimization goals and priorities
-   We all strive to be efficient, whether doing something in less time or with less space
-   But we must first prioritize to meet our various constraints

# Organize code for introducing optimizations

## Create functions to evaluate approaches and quantify optimization

-   Goal: Modular, repeatable units of code
-   Keep old functions that you've tried, even the failures
-   Create examples with `{roxygen}` and tests with `{testthat}`

```{r}
mean1 <- function(x) mean(x)
mean2 <- function(x) sum(x) / length(x)
x <- runif(1e5)
bench::mark(
  mean1(x),
  mean2(x)
)[c("expression", "min", "median", "mem_alloc")]
```

## Online resources and communities can provide existing solutions or new coding approaches

-   After exhausting options at hand, online resources can provide new approaches not thought of.

-   Alternatively, reframe the problem to find solutions

-   Talking to peers helps brainstorm solutions and reframe the problem

-   Learning broadly improves your ability to dissect problems and develop solutions

# Strategies for code optimization

## Functions, arguments, and data types should factor into code optimization for the problem at hand.

::: panel-tabset

### Use vectorisation

```{r}
X <- matrix(1:1e6,nrow=100)
bench::mark(
  colMeans(X),
  apply(X,2,function(x)sum(x) / length(x))
)[c("expression", "min", "median", "mem_alloc")]
```

### Prespecify outputs

-   Note: specifying `double` versus `numeric` in vapply regulates speed.

```{r}
X <- rep(list(1:1e4),1e4)
bench::mark(
  vapply(X,mean,double(1)),
  sapply(X,mean)
)[c("expression", "min", "median", "mem_alloc")]
```

### Test equally versus set inclusion

-   Note: `any` becomes faster than  `%in%` if `length(X)>1e7`

```{r}
X <- 1:1e8
bench::mark(
  any(X == 1450),
  1450 %in% X
)[c("expression", "min", "median", "mem_alloc")]
```

### Avoid method dispatch

```{r}
x <- runif(1e2)

bench::mark(
  mean(x),
  mean.default(x)
)[c("expression", "min", "median", "mem_alloc")]
```
:::

## Using vectorised code often leads to optimized code

-   Vectorisation: Using an R function implemented
-   Matrix algebra functions are a great example of vectorisation
-   The help documentation can lead you to a vectorised solution
-   Performing operations at the scale of your problem shows actual versus expected optimizations

## Avoiding copies

-   Whenever you use c(), append(), cbind(), rbind(), or paste() to create a bigger object, R must first allocate space for the new object and then copy the old object to its new home.

```{r}
random_string <- function() {
  paste(sample(letters, 50, replace = TRUE), collapse = "")
}
strings10 <- replicate(10, random_string())
strings100 <- replicate(100, random_string())
collapse <- function(xs) {
  out <- ""
  for (x in xs) {
    out <- paste0(out, x)
  }
  out
}
bench::mark(
  loop10  = collapse(strings10),
  loop100 = collapse(strings100),
  vec10   = paste(strings10, collapse = ""),
  vec100  = paste(strings100, collapse = ""),
  check = FALSE
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

## Case study: t-test

```{r}
m <- 1000
n <- 50
X <- matrix(rnorm(m * n, mean = 10, sd = 3), nrow = m)
grp <- rep(1:2, each = n / 2)
```

```{r}
# formula interface
system.time(
  for (i in 1:m) {
    t.test(X[i, ] ~ grp)$statistic
  }
)
# provide two vectors
system.time(
  for (i in 1:m) {
    t.test(X[i, grp == 1], X[i, grp == 2])$statistic
  }
)
```

Add functionality to save values

```{r}
compT <- function(i){
  t.test(X[i, grp == 1], X[i, grp == 2])$statistic
}
system.time(t1 <- purrr::map_dbl(1:m, compT))
```

If you look at the source code of `stats:::t.test.default()`, youâ€™ll see that it does a lot more than just compute the t-statistic.

```{r}
# Do less work
my_t <- function(x, grp) {
  t_stat <- function(x) {
    m <- mean(x)
    n <- length(x)
    var <- sum((x - m) ^ 2) / (n - 1)
    list(m = m, n = n, var = var)
  }
  g1 <- t_stat(x[grp == 1])
  g2 <- t_stat(x[grp == 2])
  se_total <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / se_total
}
system.time(t2 <- purrr::map_dbl(1:m, ~ my_t(X[.,], grp)))
stopifnot(all.equal(t1, t2))
```

This gives us a six-fold speed improvement!

```{r}
# Vectorise it
rowtstat <- function(X, grp){
  t_stat <- function(X) {
    m <- rowMeans(X)
    n <- ncol(X)
    var <- rowSums((X - m) ^ 2) / (n - 1)
    list(m = m, n = n, var = var)
  }
  g1 <- t_stat(X[, grp == 1])
  g2 <- t_stat(X[, grp == 2])
  se_total <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / se_total
}
system.time(t3 <- rowtstat(X, grp))
stopifnot(all.equal(t1, t3))
```

1000 times faster than when we started!

## Other techniques

-   [Read R blogs](http://www.r-bloggers.com/) to see what performance problems other people have struggled with, and how they have made their code faster.

-   Read other R programming books, like The Art of R Programming or Patrick Burns' [*R Inferno*](http://www.burns-stat.com/documents/books/the-r-inferno/) to learn about common traps.

-   Take an algorithms and data structure course to learn some well known ways of tackling certain classes of problems. I have heard good things about Princeton's [Algorithms course](https://www.coursera.org/course/algs4partI) offered on Coursera.

-   Learn how to parallelise your code. Two places to start are Parallel R and Parallel Computing for Data Science

-   Read general books about optimisation like Mature optimisation or the Pragmatic Programmer

-   Read more R code. StackOverflow, R Mailing List, DSLC, GitHub, etc.