---
engine: knitr
title: Measuring performance
---

## Learning objectives:

- Understand how to figure out where R is spending time executing your code
- Learn about the tools for benchmarking your code
- Implement code profiling


## Introduction

> "Before you can make your code faster, you first need to figure out whatâ€™s making it slow."


```{r echo=FALSE, fig.align='center',fig.cap="SLOW DOWN TO LEARN HOW TO CODE FASTER | credits: packtpub.com"}
knitr::include_graphics("images/23_code_faster.jpeg")
```


- **profile** your code: measure the run-time of each line of code using realistic inputs
- **experiment** with alternatives to find faster code
- **microbenchmark** to measure the difference in performance.

## Packages introduced

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(profvis)
library(bench)
library(microbenchmark)
```


## Profiling

Profiling the code is the process of gathering information about the resource
requirements of the individual calls in your code.

Profilers sample the code performance by stopping the execution of code 
every few milliseconds, gauging which process is being engaged, 
and recording the resource utilization.

Example:

```{r}
f <- function() {
  pause(0.1)
  g()
  h()
}
g <- function() {
  pause(0.1)
  h()
}
h <- function() {
  pause(0.1)
}
```

## Profiling

Profile the execution of f():

    - `profvis::pause()` is used instead of `Sys.sleep()` 
      as the latter makes R use no resources
    - profile `f()`, with `utils::Rprof()`
    
```{r}
tmp <- tempfile()
Rprof(tmp, interval = 0.1)
f()
Rprof(NULL)
writeLines(readLines(tmp))
```
    
    
## Visualising profiles

Makes easier to build up a mental model of what you need to change:

    - `profvis::profvis(expr)`
    - `utils::summaryRprof()`

```{r}
source("scripts/profiling-example.R")
profvis(f())
```

## Mini-case study: poor practice of copy-on-modify

Profiling a loop that modifies an existing variable:
most of the time is spent in 
[garbage collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) 
(i.e. releasing and reclaiming memory that is no longer needed).

```{r}
profvis::profvis({
  x <- integer()
for (i in 1:1e4) {
  x <- c(x, i)
}
})
```

You can figure out what is the source of the problem by looking at the memory column. 
In this case, **copy-on-modify** acts in each iteration of the loop creating another copy of `x`.

- All the time is being spent in the concatenation operation.
- Nearly the same amount of memory is being asked for and is being released.

## Limitations

- Profiling does not extend to C code.
    * C code can be profiled with its own tools in C IDEs.
- Anonymous functions do not show up properly.
- Lazy evaluation of function arguments makes things complicated.
    * Are things evaluated inside another function, and whose line is it anyway?


### Exercise

Where is the time being spent in this function?

```{r eval=FALSE}
profvis::profvis({
  f <- function(n = 1e5) {
  x <- rep(1, n)
  rm(x)
}
}, torture = 10)
```

    ?rm()
    
[Solution](https://advanced-r-solutions.rbind.io/measuring-performance.html)    
    
## Microbenchmarking

In software development, _microbenchmarking_ is understood as 
measuring time or other aspects of performance of a very small piece of code.
You expect the code to run microseconds, but you know that in your
larger project, you will run this thousands or millions of times.
It is particularly useful when you are considering different implementations
of that very frequently repeated step, and want to maximize performance.

```{r echo=FALSE, fig.align='center',fig.cap = "Credits: Google search-engine"}
knitr::include_graphics("images/23_microbenchmarking.jpeg")
```

## Microbenchmarking

```{r}
#| label: microbenchmark-setup
x <- runif(100)
uniroot_sqrt <- function(x) {
  solution <- x
  for(i in seq_along(x)) {
    solution[i] <- uniroot( 
      f = function(z, a) { z*z - a }, 
      interval = c(0,1), 
      tol = 5 * .Machine$double.eps,
      a = x[i]) |> 
      purrr::pluck('root')
  }
  solution
}
```

## Microbenchmarking

```{r}
#| label: microbenchmark1
mb1 <- microbenchmark(
  sqrt(x),
  x ^ 0.5,
  exp( 0.5 * log(x)),
  uniroot_sqrt(x)
)
mb1
```

The underlying `data.frame()`-ish object is only `expr` and `time`.
What you see above is the `summary()` method applied to it.
(Output of plain `mb1` differs between rendering systems;
interactively it is passed through the `print()` method.)

## Microbenchmarking

Much greater detail is provided by the `{bench}` package: `bench::mark()`.
    
    
```{r}
#| label: microbenchmark2
lb <- bench::mark(
  sqrt(x),
  x ^ 0.5,
  exp( 0.5 * log(x)),
  uniroot_sqrt(x)
)
lb
```

Again, what you see above is `summary(lb)`. 
Timing for each run is inside the S3 class components.
(Output of plain `lb` differs between rendering systems;
interactively it is passed through the `print()` method.)

## Microbenchmarking distributions

```{r}
plot(lb)
```

Results are typically heavily skewed: some calculations
may have hit "unfortunate" moments with the system was busy
with something else. Pay more attention to medians rather than means.
The `uniroot`-based calculation was so slow that `bench::mark()`
decided to stop after a few hundred runs rather than the default
`r max(lb$n_itr)` runs, so as to fit the total simulation budget of 0.5 sec.

## Other considerations

::: notes
Should this go to the next chapter "Performance" instead maybe?
:::

Things that can/will affect performance,
some are hard to measure:

- Scaling 
    * bubble sort: $O(n^2)$, actual sort: $O(n \log n)$
    * matrix inversion: $O(n^3)$ in linear algebra classes,
      $O(n^{-2.3})$ with the best algorithms
      
- I/O bottlenecks

  * CPU cache $\approx$ 1000x faster than RAM which is 
    $\approx$ 1000 faster than local HDD which is $\approx$ 
    10--100 faster than network/cloud

- Laziness and [prudence](https://duckplyr.tidyverse.org/articles/prudence.html):
when things get evaluated and put into memory

- Understand how parallel processing (`future`, `mirai`) passes objects between instances,
  depend on each others' completions, and wait for one another otherwise

## Resources

- [profvis package](https://rstudio.github.io/profvis/)
- [bench package](https://cran.r-project.org/web/packages/bench/bench.pdf)
- [solutions](https://advanced-r-solutions.rbind.io/measuring-performance.html)
